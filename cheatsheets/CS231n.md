# Checklist

 - [ ] [Check that GPU(s) are configured](##GPUs)
 - [ ] [Check the Network Capacity](##NetworkCapacity)
 - [ ] [Setup Network Layers](##NetworkLayers)
 - [ ] [Load Pretrained Models](##PretrainedModels)
 - [ ] [Data Loader and Pre-processing](##DataLoader&Preprocessing)
 - [ ] [Configure HyperParams](##HyperParams)
 - [ ] [Check Loss Function](##LossFunction)
 - [ ] [Initialize Optimizer](##Optimizer)
 - [ ] [Test Run with Small Batch](##TestRunwithSmallBatch)

## GPU(s)

#### nvidia
```
# Setup Commands
nvidia-uninstall	 # Purge Nvidia Drivers  
./NVIDIA… 			 # Install Drivers 
nvidia-xconfig

# Performance Issues
watch nvidia-smi -q . out.txt 	# check output of GPUs
```
#### cuda
```
# Setup Commands
CUDNN sudo dpkg -i libcudnn7-dev_7.6.5.32-1+cuda10.2_amd64.deb

# Check cudnn Version
nvcc --version nvcc -V cat /usr/include/x86_64-linux-gnu/cudnn_v*.h | grep CUDNN_MAJOR -A 2
```

#### pytorch

```
python -c "import torch; print(torch.cuda.is_available())"  
  
use_cuda = torch.cuda.is_available()  
device = torch.device("cuda:0" if use_cuda else "cpu")  
model = model.to(device=device)  # move the model parameters to CPU/GPU  
print('Using device:{}'.format(device))  
  
# VARIABLE  
points = Variable(points).cuda()  
Detach() # remove from computational graph  
  
# DATA PARALLEL  
if torch.cuda.device_count() > 1:  
print("Let's use", torch.cuda.device_count(), "GPUs!")  
model = nn.DataParallel(model)  
```

####  tensorflow
```
pip freeze | grep tensorflow  
python -c "import os; os.environ["CUDA_VISIBLE_DEVICES"] = "0"; import tensorflow; print(tensorflow.test.is_gpu_available())"  
python -c "import tensorflow; print(tensorflow.test.is_gpu_available())”  
  
#####################  
# tf 1  
#####################  
config_ = tf.ConfigProto()  
gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.90)  
config_ = tf.ConfigProto(gpu_options=gpu_options)  
# config_.gpu_options.allow_growth = True  
sess = tf.Session(config=config_)  
  
#####################  
# tf 2  
#####################  
import tensorflow as tf  
gpus = tf.config.experimental.list_physical_devices('GPU')  
if gpus:  
 try: tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)]) logical_gpus = tf.config.experimental.list_logical_devices('GPU') print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs") except RuntimeError as e: # Virtual devices must be set before GPUs have been initialized print(e)
```

## NetworkCapacity 

#### torchsummary
```
from torchsummary import summary  
summary(net, (config.NUM_CHANNELS, config.CROP_H, config.CROP_W))
```
#####  todo: insert network_capacity.png

#### TODO: Parameter Sharing
```
# If all neurons in a single depth slice are using the same weight vector, then the forward pass of the CONV layer can in each depth slice be computed as a convolution of the neuron’s weights with the input volume
```
#####  todo: insert parameter_sharing.png

#### TODO: FC to CONV
#####  todo: insert fc_to_conv_layer.png

## NetworkLayers

#### Overview
```
# INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC  
  
# INPUT IMAGE  
- should be divisible by 2 many times. Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512  
  
# CONV  
- use small filters (e.g. 3x3 or at most 5x5), using a stride of S=1, and crucially, padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input. (USE P=1 WHEN F=3 OR P=2 WHEN F=5)  
- stacking CONV layers with tiny filters as opposed to having one CONV layer with big filters allows us to express more powerful features of the input, and with fewer parameter (MORE MEMORY)  
- Smaller strides work better in practice (LEAVE DOWNSAMPLING TO POOLING)  
- keeping the spatial sizes constant after CONV with pooling actually improves performance (information at the borders can be “washed away” too quickly)  
  
# KERNAL SIZE (i.e. DEPTH)  
- if your input images are smaller than 128x128 use 1x1 or 3x3  
  
- if your input images are greater than 128×128 use 5X5 or 7X7  
1. learn larger spatial filters  
2. to help reduce volume size  
3. then start working with 3x3  
  
# FILTER SIZE (i.e. RECEPTIVE SIZE)  
- start small (64) to learn low level features then move to bigger sizes (2048)  
  
# STRIDE  
- strides of 2×2 as a replacement to max pooling (REPORT BETTER ACCURACY!!!)  
[e.g.] ResNet relies (2x2) strided convolution rather than max pooling  
  
# POOL  
- use max-pooling with 2x2 receptive fields (i.e. F=2), and with a stride of 2 (i.e. S=2). Note that this discards exactly 75% of the activations in an input volume (due to downsampling by 2 in both width and height).  
- the POOL layers alone are in charge of down-sampling the volumes spatially (we would have to very carefully keep track of the input volumes throughout the CNN architecture and make sure that all strides and filters “work out”)  
  
# PADDING  
1. reduce via max pooling  
2. reduce via strided convolution  
  
padding = (F-1)/2 perserves the input size!!!  
[e.g.] F=3, then use padding = 1  
[e.g.] F=5, then use padding = 2  
  
[e.g.] x = ZeroPadding2D((3, 3))(img_input)  
  
padding = "valid" (CAN REDUCE SPATIAL SIZE!!!)  
padding  = "same"
```
##### todo: insert padding.png

#### TODO: Batch Norm

#### TODO: Dropout

#### Image Processing
```  
# Set up a convolutional weights holding 2 filters, each 3x3  
w = np.zeros((2, 3, 3, 3))  
  
# The first filter converts the image to grayscale.  
# Set up the red, green, and blue channels of the filter.  
w[0, 0, :, :] = [[0, 0, 0], [0, 0.3, 0], [0, 0, 0]]  
w[0, 1, :, :] = [[0, 0, 0], [0, 0.6, 0], [0, 0, 0]]  
w[0, 2, :, :] = [[0, 0, 0], [0, 0.1, 0], [0, 0, 0]]  
  
# Second filter detects horizontal edges in the blue channel.  
w[1, 2, :, :] = [[1, 2, 1], [0, 0, 0], [-1, -2, -1]]  
  
# Vector of biases. We don't need any bias for the grayscale  
# filter, but for the edge detection filter we want to add 128  
# to each output so that nothing is negative.  
b = np.array([0, 128])  
```
##### todo: insert image_processing.png

## LoadModel

#### TODO: Weight initialization
```
layers_dims = np.hstack([input_dim, hidden_dims, num_classes])  
W = np.random.normal(0,weight_scale,(layers_dims[i],layers_dims[i+1]))  
b = np.zeros(layers_dims[i+1])
```

#### TODO: Regularization
```
# L2: Use all of its inputs a little rather than some of its inputs a lot!!!  
reg_loss += (1/2) * reg * np.sum(W*W)  
dreg = reg*W
```

#### Load pre-trained weights
```
# Freeze earlier layers and retrain softmax classifer (i.e. MaskRCNN Head vs All)
net.load_state_dict(torch.load(config.SEG_SAVED_WEIGHTS, map_location=device))  
print(f'Model loaded from {config.SEG_SAVED_WEIGHTS}!\n')
```

## DataLoader&Preprocessing

#### TODO: DataLoader

#### Image Mean
```
# Preprocessing for Gradient Descent!!!  
X_train -= np.mean(X_train, axis=0)     # Along each feature  
X_train -= np.mean(X_train)             # Across each pixel  
  
# Images are already [0, 255] (~same scale)  
var = (1/N)*np.sum((X-mu)**2, axis=0)       # Normilization  
var = 1/(N-1)*np.sum((X-mu)**2, axis=0)     # Unbiased var of the sample covariance (bessel's correction)

# PCA/Whitening  
X -= np.mean(X, axis = 0)               # zero-center  
cov = np.dot(X.T, X) / X.shape[0]       # covariance matrix  
U,S,V = np.linalg.svd(cov)  
Xrot_reduced = np.dot(X, U[:,:100])     # decorrelate the data  
Xwhite = Xrot / np.sqrt(S + 1e-5)       # divide by the eigenvalues (which are square roots of the singular values)
```

#### Todo: Image Augmentation
```
```

## HyperParams

#### TODO: Learning Rate Scheduler
```  
1. Consider a low learning rate less than 1e-3 (say 1e-4) for pre-trained models
2. If you train your network from scratch, consider a learning rate greater than or equal 1e-3
```  

##### todo: inset learning_rate.png
##### todo: inset learning_rate_decay.png

#### Batch Size

```  
- amount of “wiggle” (NOISE) in the loss is related to the batch size  
- when the batch size is the full dataset, the wiggle will be minimal because every gradient update should be improving the loss function monotonically (unless the learning rate is set too high)  
- limited by hardware  
```
##### todo: inset batch_size.png

#### Random Grid Search
```  
# Look at performance during validation and write a model checkpoint for best preformance!!!
lr, reg, hidden_size = random_search_hyperparams([0.001], [0.05, 0.1, 0.15], [50, 80, 100, 120, 150, 180, 200])  
  
# generate random hyperparameters given ranges for each of them  
def generate_random_hyperparams(lr_min, lr_max, reg_min, reg_max, h_min, h_max):  
    lr = 10**np.random.uniform(lr_min,lr_max)  
    reg = 10**np.random.uniform(reg_min,reg_max)  
    hidden = np.random.randint(h_min, h_max)  
    return lr, reg, hidden  
  
# get random hyperparameters given arrays of potential values  
def random_search_hyperparams(lr_values, reg_values, h_values):  
    lr = lr_values[np.random.randint(0,len(lr_values))]  
    reg = reg_values[np.random.randint(0,len(reg_values))]  
    hidden = h_values[np.random.randint(0,len(h_values))]  
    return lr, reg, hidden  
```
##### todo: inset random_grid_search.png

## LossFunction

```  
1. Loss: HOW SCORES AGREE W GROUND TRUTH LABEL!!!  
2. Scores: Classification on raw image pixels  
```  
#### TODO: Softmax or Cross-entropy
```  
neg_log_likelihood = -tf.reduce_mean(dist.log_prob(y_true))  
  
# Compute binary cross entropy  
loss = K.switch(tf.size(y_true) > 0,  
  K.binary_crossentropy(target=y_true, output=y_pred),  
  tf.constant(0.0))  
loss = K.mean(loss)  
  
# Cross entropy loss  
loss = K.sparse_categorical_crossentropy(target=anchor_class,  
  output=rpn_class_logits,  
  from_logits=True)  
loss = K.switch(tf.size(loss) > 0, K.mean(loss), tf.constant(0.0))
```

## Optimizer

#### TODO: 
```  
# SGD + MOMENTUM for Local Gradients (Takes Longer) !!!  
```

## TestRunwithSmallBatch

#### TODO: Gradient Checks
```  
# look at step size (h), few datapoints and kinks!  
```

#### Tensorboard  
```  
# EVENT FILE  
from torch.utils.tensorboard import SummaryWriter  
writer = tf.summary.FileWriter([logdir], [graph])  
writer = SummaryWriter()  
  
1. scalar summary (LOSS)  
writer.add_scalar('logs/train/loss', loss.item(), batch_idx+(epoch-1)*loader.batch_size)  
writer.add_scalar('logs/test/loss', loss.item(), batch_idx+(epoch-1)*loader.batch_size)  
```

## Evaluation

#### Precision and Recall  
```  
- metrics are dependent on the model score threshold that you set !!!!!  
[e.g.] DETECTION_MIN_CONFIDENCE = 0.9  
  
1. PRECISION [LOW FALSE POSITIVE]  
- refers to the percentage of your results which are REVELENT!!!  
- the percentage of your predictions are correct (TRUE POSITIVE / ALL POSITIVES)  
- a precision score of close to 1.0 then there is a high likelihood that whatever the classifier predicts as a positive detection is in fact a correct prediction  
  
2. RECALL [LOW FALSE NEGATIVE]  
- refers to the percentage of total relevant results CORRECTLY!!! classified by your algorithm  
- how good you find all the positives (TRUE POSITIVE / TRUE POSITIVE + FN)  
- a recall score close to 1.0 then almost all objects that are in your dataset will be positively detected by the model  
  
Type I & II Error  
[e.g.]   
- the null hypothesis in this case is that this call is a hoax  
- if Jack would have believed the stranger and provided his bank details, and the call was in fact a hoax, he would have committed a TYPE I ERROR  
  
  
Trade-off [F1-Score]  
- If you have to recall everything, you will have to keep generating results which are not accurate, hence lowering your precision  
- Not possible to maximize both these metrics at the same time  
- F1 is the harmonic mean of precision and recall  
```

##### todo: inset precision_and_recall.png

#### TODO: mIOU

#### TODO: Fwb
<!--stackedit_data:
eyJoaXN0b3J5IjpbNzU5MTg3MTc0LDE1MDc1Mzg0MjAsMzYxMT
E4NTE2LDEwOTI3NDgzMCwtMTY0MDA2MzE1OCwxNTg5ODA0MDU5
LC0zMTgxNjQ5OTksLTc0NTE2Mzk0Niw3OTkzMzY4ODMsMjY3NT
kwMTY1LC0yMDg4NzQ2NjEyXX0=
-->